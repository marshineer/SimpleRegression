# SimpleRegression
Collection of Simple Regression Analyses

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Project Goal
This project was undertaken to learn more about regression analyses using the scikit-learn library. It consists of five notebooks analyzing three datasets.

## Project Descriptions

### Heating and Cooling Load Predictor

This was the first project undertaken. The purpose was to get some practice exploring a dataset, pre-processing and to use the lasypredict library for the first time. In hindsight, I now realize that two of the features are categorical and should have been encoded before running machine learning on them. Although the data was not formatted properly and the models were not tuned at all, they still achieved an $R^2$ score of 0.90. This indicates that this is likely a relatively easy regression task (at least compared to the forest fire data set analyzed later).

### Power Plant Output Predictor

Here, I performed data cleaning and a brief exploratory data analysis (EDA), where I observed the relationships between features and the effects of whitening them. The pairplots clearly showed how whitening eliminates skewness, resulting in much more Gaussian distributions after the transformation. Subsequently, the lazypredict library was used to estimate the best model, and a hyperparamter search was performed using manual looping (this is improved upon by utilizing sklearn's GridSearchCV in the next analysis). The effects of changing one parameter (while holding all others at the optimal values) is illustrated. It appears the effects of the hyperparameters is not particularly strong (i.e. many models perform relatively well), so there is not a clear maximum in these curves. 

The last part of the notebook is a neural network analysis. I implemented a basic neural network, with three hidden layers. This was probably overkill, although the fact that both the training and test losses are similar, it does not seem to be overfitting. The neural network does not perform better than the other machine learning models either though. Given that the dataset seems to be fairly predictable, I would have thought that a neural network would be able to make nearly a perfect regression prediction. This likely indicates that I am not constructing the model correctly, and need to get more experience in the practicalities of building and training these networks.

### Forest Fire Burn Area Dataset

I divided the work on this datasest into three parts.

1. Data pre-processing and EDA: There's not really much here. I checked for NaNs and missing values, looked at the duplicate values, and converted the categorical features to numerical values. However, I hadn't known yet that categorical features need to be encoded for regression analysis, so that step is missing from this pre-processing. I ran a pandas profile_report() on the dataset to get an overview, and to point me in the direction of things I should look out for. This identified the feature "rain" and the target variable "area" as having large proportions of zeros. However, I left this to be addressed in a later analysis. The exploration consists of a pairplot and a correlation plot, which show some basic relationships between the features. This exploration is lacking creativity though, and a much better analysis can be seen [here](https://doi.org/10.3390/su141610107). This is an area of data analysis that I would like to improve on.
- Checked for NaNs and missing values
- Looked at the duplicate entries. They seemed reasonable so did not remove due to lack of domain knowledge.
- Converted categorical features to numerical values. Was not yet aware of categorical feature encoding for regression analsysis, so this is missing.
- Ran a pandas profile_report(). Identified that "rain" and "area" have high proportions of zeros (addressed in a later analysis)
- Exploration consists of just a pairplot and correlation plot, showing basic relationships between features. 
- A more creative analysis can be found [here](https://doi.org/10.3390/su141610107). This is an area of data analysis that I would like to improve on.

2. Feature selection: This was a short analysis investigating feature selection. The first part uses the sklearn function SelectKBest to score and rank the features according to their cross-correlative relationships. Since the features are continuous, they cannot be scored using mutual information, which is the other scoring metric available in that function. A random forest regressor is then trained on the full set of features, and the feature_importances_ attribute is used to compare these rankings with those from the SelectKBest function. Interestingly, there is little agreement between the two rankings. Both ranked temp strongly as the most important feature, but after that, there is little consensus. The importance of features was investigated further by sequentially dropping features and comparing the model performance using k-fold cross-validation, as measured by the $R^2$ score. The features were dropped in two orders: using the SelectKBest ranking and the built-in feature importance attribute. For most features, there was little impact on the average $R^2$, although the standard deviation wawas quite large, as scores varied greatly across folds. However, in both cases, the mean $R^2$ strongly declined once the DMC feature was removed. This is a good indication that this feature is important to model performance, and these are the type of clues I will look for in further feature selections. It should be noted that I would imagine feature importance is at least somewhat dependent on the model class being used. Therefore, in more complex problems, feature selection will have to be part of a larger model selection pipeline. I also inadvertently discovered that failing to shuffle the data before performing cross-validation greatly reduced the model performance. This could indicate the data is not well mixed with regard to the target variable, or that there are some ordinal characteristics in the data that bias the model when not accounted for. 

3. Burn area prediction: This was the most involved analysis of the dataset, incorportating all aspects that I have worked on up to this point. The categorical features are encoded using a binary encoding. Since scikit-learn does not have a built-in binary encoding function, I wrote one. Additionally, I wrote functions to whitening the data, as well as balancing the dataset with regard to particular features, in this case "month". This feature is balanced in three ways: upsampling, dividing the majority class into subsets, and a mixture of the two. Following this, three different model classes are trained using k-fold cross-validation and a shuffling scheme to avoid the issues identified during feature analysis. The models are improved by performing a hyperparameter search using scikit-learn's GridSearchCV method. Then the training and test scores are compared. Unsurprisingly, upsampling results in overfitting during training, but does not significantly improve test performance. Splitting the majority class performed worse, although this could be due to the relatively small training sets that remained after splitting the data. None of the models performed particularly well, with the best hovering around $R^2 \approx 0$, indicating a trivial fit. This means that simply always predicting the mean target value would perform as well as the trained models. Therefore, a second approach was undertaken.

Next, I tried implementing a two part model, where the data is first labelled as either "fire" and "no fire", and classified based on this. For points where the model indicates a fire occurs, a second model would perform a regression analysis to predict the burn area. This accounted for the zero imbalance in the target variable. Unfortunately, the models trained to classify on whether a fire occurs performed little better than chance. Therefore, I abandoned the second part of the model, as it would be irrelevant while the classification task fails. In order to determine whether there were better ways to perform this analysis, I looked online for other people's analyses of this dataset. Most of what I found either consisted of analyses where they agreed with my assessment that the data does not contain enough information for proper classification, those where they performed some kind of transformation or relabelling of the data before applying maching learning, or those where there was either an obvious oversight or what I would consider flawed methodology. A detailed description of my findings can be read in the notebook.
